\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\newcommand{\mse}{\operatorname{mse}}
\newcommand{\bias}{\operatorname{bias}}
\newcommand{\se}{\operatorname{se}}
\newcommand{\Bernoulli}{\operatorname{Bernoulli}}
\newcommand{\Bin}{\operatorname{Bin}}
\newcommand{\Poisson}{\operatorname{Poisson}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\MLE}{\operatorname{MLE}}

\newcommand{\F}{\mathcal{F}}


\usepackage{enumitem}

\usepackage{graphicx}

\usepackage{subcaption}

\usepackage{minted}

\usepackage{hyperref}

\begin{document}

\tableofcontents

\newpage

\section{General Probability Theory}
\begin{enumerate}
\item Using the properties of Definition 1.1.2 for a probability mea-
measure $P$, show the following.
\begin{enumerate}
	\item If $A \in \F, B \in \F$, and $A \subseteq B$ then $P(A) \leq P(B)$.
	\item If $A \in \F$ and $\{A_n\}_{n = 1}^\infty$ is a sequence of sets in $\F$ with
	$$
	\lim_{n \rightarrow \infty} P(A_n) = 0
	$$
	and $A \subseteq A-n$ for every $n$, then $P(A) = 0$. (This property was used implicitly in Example 1.1.4 when we argued that the sequence of all heads, and indeed any particular sequence, mus have probability zero.)
\end{enumerate}
	\item The infinite coin-toss space $\Omega_\infty$ of example 1.1.4 is uncountable infinite. Suppose that were was a sequential list
	$$
	\omega^{(i)} = \omega^{(i)}_1 \omega^{(i)}_2 \omega^{(i)}_3 \dots
	$$
	where $i = 1, 2, \dots$ of all elements of $\Omega_\infty$, i.e. such that
	$$
	\bigcup_i \omega^{(i)} = \Omega_\infty.
	$$
	An element that does not appear in this list is the sequence whose first component is $H$ if $\omega_1^{(1)}$ is $T$ and is $T$ if $\omega_1^{(1)}$ is $H$, and so on. As such there is no such sequence $(\omega^{(i)})$.

	Now consider the set $A \subseteq \Omega_\infty$ such that for all $\\omega \in A$ the elements $\omega_{2k - 1}$ and $\omega_{2k}$ are the same for all $k \geq 0$.
		\begin{itemize}
			\item Show that $A$ is uncountably infinite.
			\item Show that, when $0 < p < 1$, we have $P(A) = 0$.
		\end{itemize}
	\item Consider the set function $P$ defined for every subset of $[0, 1]$ by $P(A) = 0$ if $|A| < \infty$ and $P(A) = \infty$ if $|A| = \infty$. Show that $P$ satisfies (1.1.3)-(1.1.5), but $P$ does not have the countable additivity property (1.1.2). We see then that the finite additivity property (1.1.5) does nto imply the countable additivity property (1.1.2).
	\item
		\begin{enumerate}
			\item Construct a standard normal random variable $Z$ on the probability space $(\Omega_\infty, \F_\infty, P)$ of example 1.1.4 under the assumptionm that the probability for head is $p = 1 / 2$. (Hint: Consider Examples 1.2.5 and 1.2.6).
			\item Define a sequencec of random variables $\{Z_n\}_{n = 1}^\infty$ on $\Omega_\infty$ such that $Z_n \rightarrow Z$ pointwise, and, for each $n$, $Z_n$ only depends on the first $n$ coint tosses. (This gives us a procedure for approximating a standard normal random variable by random variables generated by a finite number of coin tosses, a useful algorithm for Monte Carlo simulation.)
		\end{enumerate}
	\item When dealing with double Lebesgue integrals, just as with doubel Riemann integrals, the order of integration can be reversed. The only assumption required is that the function being integrated be either nonnegative or integrable. Here is an application of this fact.

	Let $X$ be a nonnegative random variable with cumulative distribution function $F(x) = P(X \leq x)$. Show that
	$$
	EX = \int_0^\infty (1 - F(x)) dx
	$$
	by showing that
	$$
	\int_\Omega \int_0^\infty 1_{[0, X(\omega))}(x) dx dP(\omega)
	$$
	is equal to both $EX$ and
	$$
	\int_0^\infty (1 - F(x))dx.
 	$$
 	\item Let $u$ be a fixed number in $\mathbb{R}$, and define the convex function
 	$$
 	\varphi(x) = e^{ux}
 	$$
 	for all $x \in \mathbb{R}$. Let $X$ be a normal random variable with mean $\mu = EX$ and standard deviation
 	$$
 	\sigma = (E(X - \mu))^{1 / 2}.
 	$$
 	\begin{enumerate}
 		\item Verify that
 		$$
 		Ee^{uX} = e^{u\mu + \frac{1}{2} u^2 \sigma^2}.
 		$$
 		\item Verify that Jensen's inequality holds (as it must):
 			$$
 			E\varphi(X) \geq \varphi(EX).
 			$$
	\end{enumerate}
	\item For each positive integer $n$, deinfe $f_n$ to be normal density with mean zero and variance $n$, i.e.,
	$$
	f_n(x) = \frac{1}{\sqrt{2n\pi}}e^{- x^2 / 2n}.
	$$
	\begin{enumerate}
	\item Determine the pointwise limit
	$$
	f(x) = \lim_{n \rightarrow \infty} f_n(x).
	$$
	\item Calculate
	$$
	\lim_{n \rightarrow \infty} \int_{-\infty}^\infty f_n(x) dx.
	$$
	\item Note that
	$$
	\lim_{n \rightarrow \infty} \int_{-\infty}^\infty f_n(x) dx \neq \int_{-\infty}^\infty f(x) dx.
	$$
	Explain why this does not violate the Monotone Convergence Theorem, Theorem 1.4.5.
	\end{enumerate}
	\item 
	\item 
	\item 
	\item 
	\item 
	\item 
	\item 
	\item 
\end{enumerate}

\section{Information and Conditioning}
\begin{enumerate}
	\item Let $(\Omega, \F, P)$ be a general probability space, and suppose a random variable $X$ on this space is measureable with respect to the trivial $\sigma$-algebra $\mathcal{F}_0 = \{\emptyset, \Omega\}$. Show that $X$ is not random (i.e., there is a constant $c$ such that $X(\omega) = c$ for all $\omega \in \Omega$). Such a random variable is called degenerate.
	\item
	\item Let $X$ and $Y$ be independent standard random variables. Let $\theta$ be a constant, and define random variables
	$$
	\begin{aligned}
	V = X \cos \theta + Y \sin \theta,&& W = -X \sin \theta + Y \cos \theta.
	\end{aligned}
	$$
	Show that $V$ and $W$ are independent standard normal random variables.
	\item 
	\item Let $(X, Y)$ be a pair of random variables with joint density function
	$$
	f_{X, Y}(x, y) = \frac{2 |x| + y}{\sqrt{2\pi}}\exp\left( - \frac{(2|x| + y)^2}{2} \right)
	$$
	for $y \geq -|x|$ and $f_{X, Y}(x, y) = 0$ otherwise. How that $X$ and $Y$ are standard normal random variables and that they are uncorrelated but not independent.
	\item 
	\item
	\item Let $X$ and $Y$ be integrable random variables on a probability space $(\Omega, \F, P)$. Then $Y = Y_1 + Y_2$, where $Y_1 = E[Y|X]$ os $\sigma(X)$-measurable and $Y_2 = Y - E[Y|X]$. Show that $Y_2$ and $X$ are uncorrelated. More generally, show that $Y_2$ is uncorrelated with every $\sigma(X)$-measureable random variable.
	\item
	\item
	\item
		\begin{enumerate}
			\item Let $X$ be a random variable on a probability space $(\Omega, \F, P)$, and let $W$ be a nonegative $\sigma(X)$-measureable random variable. Show there exists a function $g$ such that $W = g(X)$. (Hint: Recall that every set in $\sigma(X)$ is of the form $\{X \in B\}$ for some Borel set $B \subseteq \mathbb{R}$. Suppose first that $W$ is the indicator of such a set, and then use the standard machine.)
			\item Lett $X$ be a random variable on a probability space $(\Omega, \F, P)$, and let $Y$ be a nonnegative random variable on this space. We do not assume that $X$ and $Y$ have a joint density. Nonetheless, show there is a function $g$ such that $E[Y|X] = g(X)$.
		\end{enumerate}
\end{enumerate}

\section{Brownian Motion}

\section{Stochastic Calculus}
\section{Risk-Neutral Pricing}
\section{Connections with Partial Differential Equations}
\section{Exotic Options}
\section{American Derivative Securities}
\section{Change of NumÃ©raire}
\section{Term-Structure Models}
\section{Introduction to Jump Processes}

\end{document}