\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{enumitem}

\usepackage{graphicx}

\begin{document}

\tableofcontents

\section{Data Manipulation}

\section{Data Preprocessing}

\section{Linear Algebra}
\begin{enumerate}
    \item Prove that the transpose of the transpose of a matrix is the matrix itself: \\
    $$(A^T)^T = A$$
    	\begin{itemize}
    		\item The transpose per definition satisfies $(A^T)_{ij} = A_{ji}$ for all $i, j$. As such $((A^T)^T)_{ij} = (A^T)_{ji} = A_{ij}$, which shows that $(A^T)^T = A$.
    	\end{itemize}

    \item Given two matrices $A$ and $B$, show that sum and transposition commute: \\
    $$(A + B)^T = A^T + B^T$$
    	\begin{itemize}
    		\item Let $i, j$ be arbitrary valid indiced then
    		$$((A + B)^T)_{ij} = (A + B)_{ji} = A_{ji} + B_{ji} = (A^T)_{ij} + (B^T)_{ij}$$
    		holds, this proves that $(A + B)^T = A^T + B^T$.
    	\end{itemize}

    \item Given any square matrix $A$, is $A + A^T$ always symmetric? Can you prove the result by using only the result of the previous two exercises?
    	\begin{itemize}
    		\item A square matrix $B$ is symmetric if and only if $B^T = B^T$. In view of the previous two exercises the symmetry of $A + A^T$ immediately follows:
    		$$
    		(A + A^T)^T \overset{2}{=} A^T + (A^T)^T \overset{1}{=} A^T + A = A + A^T.
    		$$ 
    		where in the last step we have used that matrix addition is commutative.
    	\end{itemize}

    \item We defined the tensor $X$ of shape $(2, 3, 4)$ in this section. What is the output of \texttt{len}(X)? Write your answer without implementing any code, then check your answer using code.
    	\begin{itemize}
    		\item I assume that internally the tensor $X$ is represented as $((\mathbb{R}^4)^3)^2$, which would then give the length as $2$.
    	\end{itemize}

    \item For a tensor \texttt{X} of arbitrary shape, does \texttt{len}(X) always correspond to the length of a certain axis of \texttt{X}? What is that axis?

    \item Run \texttt{A / A.\texttt{sum}(axis=1)} and see what happens. Can you analyze the reason?

    \item When traveling between two points in downtown Manhattan, what is the distance that you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally?
    	\begin{itemize}
    		\item This problem wants me to state the manhattan distance:
    		$$
    		d((x_1, \dots, x_n), (y_1, \dots, y_n)) = \sum_{i = 1}^n |x_i - y_i|.
    		$$
    	\end{itemize}

    \item Consider a tensor with shape $(2, 3, 4)$. What are the shapes of the summation outputs along axis $0$, $1$, and $2$?

    \item Feed a tensor with $3$ or more axes to the $\texttt{linalg.norm}$ function and observe its output. What does this function compute for tensors of arbitrary shape?

    \item Define three large matrices, say $A$, $B$, and $C$, for instance initialized with Gaussian random variables. You want to compute the product $ABC$. Is there any difference in memory footprint and speed, depending on whether you compute $AB$ or $BC$ first? Why?
    \begin{itemize}
    	\item Let $A \in \mathbb{R}^{r \times s}, B \in \mathbb{R}^{s \times t}$ and $C \in \mathbb{R}^{t \times u}$. We are assuming the naive matrix multiplication algorithm is used, then to calculate $A \cdot B \in \mathbb{R}^{r \times t}$ we need $O(rst)$ steps and similiarly for $B \cdot C \in \mathbb{R}^{s \times u}$ we need $O(s t u)$ steps. To then calculate $(A \cdot B) \cdot C$ we need $O(rtu)$ steps and to calculate $A \cdot (B \cdot C)$ we need $O(rsu)$. As such in total we need $O(rst + rtu) = O(rt(u + s))$ to calculate $(A \cdot B) \cdot C$ and $O(stu + rsu) = O(su(t + r))$.
    \end{itemize}

    \item Define three large matrices, say $A$, $B$, and $C$. Is there any difference in speed depending on whether you compute $AB$ or $BA$ first? Why? What changes if you initialize $B$ without cloning memory? Why?

    \item Define three matrices, say $A$, $B$, and $C$. Constitute a tensor with $3$ axes by stacking $A$, $B$, and $C$. What is the dimensionality? Slice out the second coordinate of the third axis to recover $B$. Check that your answer is correct.
\end{enumerate}


\section{Calculus}
\begin{enumerate} 
\item So far we took the rules for derivatives for granted. Using the definition and limits prove the properties for (i) $f(x) = c$, (ii) $x^n$, (iii) $e^x$ and (iv) $\log x$.
	\begin{itemize}
	\item We note that $f_1(x) = c$ satisfies $f_1(x) - f_1(x_0) = 0$ for all $x, x_0 \in \mathbb{R}$, which allows us to calculate:
	$$
	\lim_{x \rightarrow x_0} \frac{f(x) - f(x_0)}{x - x_0} = \lim_{x \rightarrow x_0} \frac{c - c}{x - x_0} = 0.
	$$
	\item Note that $f_2(x) = x^n$ is a polynomial and $F(x, y) = x^n - y^n$ is a polynomial in two variables which satisfies $F(x, x) = 0$ as such we can factor out $x - y$ from $F(x, y)$, it can inductively be shown that $F(x, y) = (x - y) \sum_{i = 0}^{n - 1} x^i y^{n - 1 - i}$. With this we can now calculate
	\begin{align*}
	\lim_{x \rightarrow x_0} \frac{f(x) - f(x_0)}{x - x_0} &= \lim_{x \rightarrow x_0} \frac{x^n - x_0^n}{x - x_0} \\
	&= \lim_{x \rightarrow x_0} \frac{(x - x_0)\left(\sum_{i = 0}^{n - 1} x^i x_0^{n - 1 - i}\right)}{x - x_0} \\
	&= \lim_{x \rightarrow x_0} \sum_{i = 0}^{n - 1} x^i x_0^{n - 1 - i} \\
	&= \sum_{i = 0}^{n - 1} x_0^{n - 1} = \left(\sum_{i = 0}^{n - 1} 1\right) x_0^{n - 1} = n x_0^{n - 1}.
	\end{align*}
	\item This problem depends on which representation of the exponential is used. In the book they defined it via its differential equation, i.e. $f_3(x) = e^x$ is the unique solution of $y' = y$ with initial value $y(0) = 1$, but then there is nothing to show here. Instead we'll use the power series representation
	$$
	\sum_{n = 0}^\infty \frac{x^n}{n!}
	$$
	of $f_3(x)$. It is easily verifiable that this power series is absolutely convergent, which allows us to show that
	\begin{align*}
	f_3(x) - f_3(x_0) &= \sum_{n = 0}^\infty \frac{x^n}{n!} - \sum_{n = 0}^\infty \frac{x_0^n}{n!} \\
	&= \sum_{n = 0}^\infty \frac{x^n - x_0^n}{n!} = (x - x_0) + \sum_{n = 2}^\infty\frac{x^n - x_0^n}{n!}
	\end{align*}
	holds. We can use the calculation for $f_2(x)$ so write this as
	\begin{align*}
	f_3(x) - f_3(x_0) &= (x - x_0) + (x - x_0) \sum_{n = 2}^\infty \frac{\sum_{j = 0}^{n - 1} x^j x_0^{n - 1 - j}}{n!} \\
	&= (x - x_0) \left(1 + \frac{x + x_0}{2} + \dots \right)
	\end{align*}
	In total this proves that
	$$
	f_3'(x_0) = \lim_{x \rightarrow x_0} \left(1 + \frac{x + x_0}{2} + \dots \right) = \sum_{j = 0}^\infty \frac{x_0^j}{j!} = f_3(x_0)
	$$
	\item Consider $f_4(x) = \log(x)$ and note that $f_3 \circ f_4 = \operatorname{id}$. Using the chain rule then yields
	$$
	1 = f_3' \circ f_4 \cdot f_4'
	$$
	and because $f_3' = f_3$ this reduces to
	$$
	f_4' = \frac{1}{f_3 \circ f_4}
	$$
	so that
	$$
	f_4'(x) = \frac{1}{x}.
	$$
	\end{itemize}
\item In the same vein, prove the product, sum, and quotient rule from first principles.
	\begin{itemize}
	\item Using a similiar construction to the triangle inequality we can write
	\begin{align*}
	(fg)(x) - (fg)(y) &= f(x)g(x) - f(y)g(y) \\
	&= f(x)g(x) - f(x)g(y) + f(x)g(y) - f(y)g(y) \\
	&= f(x)(g(x) - g(y)) + (f(x) - f(y))g(y),
	\end{align*}
	from which we can conclude
	\begin{align*}
	\lim_{x \rightarrow y} \frac{fg(x) - fg(y)}{x - y} &= \lim_{x \rightarrow y} \frac{f(x)(g(x) - g(y)) + (f(x) - f(y))g(y)}{x - y} \\
	&= \lim_{x \rightarrow y} f(x) \frac{g(x) - g(y)}{x - y} + \frac{f(x) - f(y)}{x - y} g(y) \\
	&= f(y) g'(y) + f'(y)g(y).
	\end{align*}
	This can also be expressed as
	$$
	(fg)' = f'g + fg',
	$$
	which is what was to be shown.
	\item The sum formula immediately follows from the linearity of $\lim$.
	\item The quotient formula immediately follows from the product rule by noting that $\frac{f}{g} = f \cdot \frac{1}{g}$ as well as $\left(\frac{1}{g(x)}\right)' = -\frac{g'(x)}{g^2(x)}$. To see this apply the product rule on $1 = g(x) \frac{1}{g(x)}$.
	\end{itemize}
\item Prove that the constant multiple rule follows as a special case of the product rule.
	\begin{itemize}
	\item $(cf(x))' = c' f(x) + c f'(x)$, we have already shown that $c' = 0$ so that $(cf)' = cf'$.
	\end{itemize}
\item Calculate the derivative of $f(x) = x^x$. 
	\begin{itemize}
	\item We assume that $x > 0$ then $f(x) = x^x = e^{x \log(x))}$. Let $u(x) = x \log(x)$ then $u'(x) = \log(x) + x \cdot \frac{1}{x} = \log(x) + 1$. This allows us to write $f(x) = e^{u(x)}$, the derivative of $f$ is then, by using the chain rule and the fact that $\frac{d}{dx} e^x = e^x$, given by
	$$
	f'(x) = u'(x) e^{u(x)} = (1 + \log(x))e^{x \log(x)} = (1 + \log(x))x^x.
	$$
	\end{itemize}
\item What does it mean that $f'(x) = 0$ for some $x$? Give an example of a function $f$ and a location $x$ for which this might hold.
	\begin{itemize}
	\item Depending on the neighboring values (or if it's $C^2$ on the sign of the second derivative), that $x$ is either a minimizer ($x = 0$ for $f(x) = x^2$), a maximizer ($x = 0$ for $f(x) = -x^2$) or a saddle point ($x = 0$ for $f(x) = x^3$).
	\end{itemize}
\item Plot the function $y = f(x) = x^3 - \frac{1}{x}$ and plot its tangent line at $x = 1$.
	\begin{itemize}
	\item The derivative of $f$ is given by $f'(x) = 3x^2 + \frac{1}{x^2}$, evaluated at $x = 1$ this yields a slope of $f'(1) = 3 + 1 = 4$, so the tangent line is given by $y(x) = 4x + b$ where $b$ is such thatt $y(1) = 4 + b = f(1) = 0$ so that $b = -4$. In total this means the tangent line at $x = 1$ is given by
	$$
	y(x) = 4x - 4.
	$$
	\begin{center}
    	\includegraphics[width=0.6\textwidth]{Images/4_6.png}
    \end{center}
	\end{itemize}
\item Find the gradient of the function $f(x) = ||x||_2$? What happens for $x = 0$?
	\begin{itemize}
	\item Note that if $x = (x_1, \dots, x_n)$ then $g(x) := f(x)^2 = ||x||_2^2 = \sum_{i = 1}^n x_i^2$. It follows that for $x \neq 0$ the corresponding partial derivates are simply given by
	$$
	\frac{\partial}{\partial x_j} f(x)^2 = \frac{\partial}{\partial x_j} \sum_{i = 1}^n x_i^2 = 2 x_i.
	$$
	This shows that $\nabla g(x) = 2 x$. Furthermore $\frac{\partial}{\partial x_j} g(x) = \frac{\partial}{\partial x_j} f(x)^2 = 2 \frac{\partial f}{\partial x_j} \cdot f(x)$. With that we can calculate
	$$
	2x = 2 f(x) \nabla f \iff \nabla f = \frac{x}{f(x)} = x / ||x||.
	$$
	Note that this construction breaks down at $x = 0$ as we would divide by $0$. But this is not surprising because $x \mapsto \sqrt{x}$ is not differentiable at $0$ and $f(x) = \sqrt{g(x)}$.
	\end{itemize}
\item Can you write out the chain rule for the case where $u = f(x, y, z)$ and $x = x(a, b), y = y(a, b)$ and $z = z(a, b)$?
	\begin{itemize}
	\item We can write $u(a, b) = f(x(a, b), y(a, b), z(a, b))$ and with that
	$$
	\frac{\partial}{\partial a} u(a, b) = \frac{\partial f}{\partial x}(\dots) \cdot \frac{\partial x}{\partial a} + \frac{\partial f}{\partial y}(\dots) \cdot \frac{\partial y}{\partial a} + \frac{\partial f}{\partial z}(\dots) \cdot \frac{\partial z}{\partial a},
	$$
	which can be expressed as
	$$
	(\nabla f)(x(a, b), y(a, b), z(a, b)) \cdot \nabla_a (x, y, z).
	$$
	Similiarly once can show taht
	$$
	(\nabla f)(x(a, b), y(a, b), z(a, b)) \cdot \nabla_b (x, y, z).
	$$
	With that it follows that
	$$
	\nabla u(a, b) = (\nabla f, \nabla f)^t \cdot (D(x, y, z)).
	$$
	\end{itemize}
\item Given a function $f(x)$ that is invertible, compute the derivative of its inverse $f^{-1}(x)$. Here we have that $f^{-1}(f(x)) = x$ and conversely $f(f^{-1}(y)) = y$. Hint: Use these properties in your derivation.
	\begin{itemize}
		\item We can calculate
		$$
		1 = \frac{\partial}{\partial x} x = \frac{\partial}{\partial x} f^{-1}(f(x)) = (f^{-1})'(f(x)) f'(x),
		$$
		which can be rewritten as
		$$
		(f^{-1})'(f(x)) = \frac{1}{f'(x)}.
		$$
		If we write $y = f(x)$ this becomes
		$$
		(f^{-1})'(y) = \frac{1}{f'(x)}.
		$$
	\end{itemize}
\end{enumerate}

\section{Automatic Differentiation}
\begin{enumerate}
\item Why is the second derivative much more expensive to compute than the first derivative?
	\begin{itemize}
		The derivative can be approximated by sampling two points, the second derivative needs more points. If we consider the second derivative as the derivative of the derivative then we need to sample two points at each of the original sampling points, which more than doubles the workload. Note that the second derivative is the average rate of change of the rate of change of the two sample points, symbolically:
		$$
		f'(x_0) \sim f(x_0 + \varepsilon) - f(x_0 - \varepsilon)
		$$
		and
		\begin{align*}
		f'(x_0 + \varepsilon) = f(x_0 + 2\varepsilon) - f(x_0),&&f'(x_0 - \varepsilon) = f(x_0) - f(x_0 - 2\varepsilon).
		\end{align*}
		From this it can be seen that the second derivative can be expressed as
		\begin{align*}
		f''(x_0) &= f'(x_0 + \varepsilon) - f'(x_0 - \varepsilon) \\
		&= f(x_0 + 2\varepsilon) - f(x_0) - (f(x_0) - f(x_0 - 2\varepsilon)) \\
		&= f(x_0 + 2\varepsilon) - 2f(x_0) + f(x_0 - 2\varepsilon)).
		\end{align*}
		There are much better choice for sampling points to get faster numerical convergence, but this is beyond the scope of what we're doing here. 
	\end{itemize}
\item After running the function for backpropagation, immediately run it again and see what happens. Why?
\item In the control flow example where we calculate the derivative of d with respect to a, what would happen if we changed the variable a to a random vector or a matrix? At this point, the result of the calculation f(a) is no longer a scalar. What happens to the result? How do we analyze this?
\item Let $f(x) = \sin(x)$. Plot the graph of $f$ and its derivative $f'$. Do not exploit the fact that $f'(x) = \cos(x)$ but rather use automatic differentiation to get the result.
\item Let $f(x) = ((\log x^2) \cdot \sin x) + x^{-1}$. Write out a dependency graph tracing results from $x$ to $f(x)$.
\item Use the chain rule to compute the derivative $f'$ of the aforementioned function, placing each term on the dependency graph that you constructed previously.
\item Given the graph and the intermediate derivative results, you have a number of options when computing the gradient. Evaluate the result once starting from $x$ to $f$ and once from $f$ tracing back to $x$. The path from $x$ to $f$ is commonly known as \textit{forward differentiation}, whereas the path from $f$ to $x$ is known as \textit{backward differentiation}.
\item When might you want to use forward differentiation and when backward differentiation? Hint: consider the amount of intermediate data needed, the ability to parallelize steps, and the size of matrices and vectors involved.
\end{enumerate}

\section{Probability and Statistics}
This might be a bit too technical for the course, but to remind myself of the background. Let $(\Omega, \mathcal{A}, P)$ be a probability space, this is a measure space (i.e. $\Omega$ is some set, $\mathcal{A}$ is a sigma algebra on $\Omega$ and $P$ is a measure, i.e. a $\sigma$-additive function $P : \mathcal{A} \rightarrow [0, +\infty)$ which satisfies $P(\emptyset) = 0$), where the measure is a probability function, i.e. $P(\Omega) = 1$. Recall that this means that probability theory is only interested in the theory of finite measures.

A random variable is a measureable function $X : \Omega \rightarrow \mathbb{R}$. We call the elements of the sigma algebra an event (and the sigma-algebra itself the event space) and the elements of $\Omega$ a sample (and $\Omega$ itself a sample space). The expression
$$
P\left(\lim_{n \rightarrow \infty} X_n = \alpha\right) = p
$$
is simply shorthand for
$$
P\left(\{\omega \in \Omega : \lim_{n \rightarrow \infty} X_n(\omega) = \alpha\}\right) = p.
$$

\begin{enumerate}
	\item Give an example where observing more data can reduce the amount of uncertainty about the outcome to an arbitrarily low level.
	\begin{itemize}
		\item Consider flipping a weighted coin, whose probability to land on heads is given by $p_H \in (0, 1)$. Let $(X_i)_{i \in \mathbb{N}}$ be a sequence of random variables where $X_i \in \{0, 1\}$ and is $1$ if the $i$th flip landed on heads. Counting the outcome of the first $n$ flips yields the random variable
		$$
		\tilde{X}_n := \sum_{i}^n X_i.
		$$
		Note that
		$$
		P\left(\lim_{n \rightarrow \infty} \frac{\tilde{X}_n}{n} \neq p_H\right) = 0,
		$$
		or if we consider the running average $Y_n := \frac{1}{n} \tilde{X}_n$ this can be expressed as
		$$
		P\left(\lim_{n \rightarrow \infty} Y_n = p_H\right) = 1.
		$$
	\end{itemize}

	\item Give an example where observing more data will only reduce the amount of uncertainty up to a point and then no further. Explain why this is the case and where you expect this point to occur.
	\item We empirically demonstrated convergence to the mean for the toss of a coin. Calculate the variance of the estimate of the probability that we see a head after drawing $n$ samples.
		\begin{enumerate}
			\item How does the variance scale with the number of observations?
			\item Use Chebyshev's inequality to bound the deviation from the expectation.
			\item How does it relate to the central limit theorem?
		\end{enumerate}
	\item Assume that we draw samples from a probability distribution with zero mean and unit variance. Compute the averages
	$$
	z_m = m^{-1} \sum_{i = 1}^m x_i.
	$$
	Can we apply Chebychev's inequality for every $z_m$ independently? Why not?
	\item Given two events with probability $P(\mathcal{A})$ and $P(\mathcal{B})$ compute upper and lower bounds on $P(\mathcal{A} \cup \mathcal{B})$ and $P(\mathcal{A} \cap \mathcal{B})$. Hint: graph the situation using a Venn diagram.
		\begin{itemize}
			\item Using a Venn diagram we can immediately see that
			$$
			\max\{P(A), P(B) \} \leq P(A \cup B) = P(A) + P(B) - P(A \cap B) \leq P(A) + P(B),
			$$
			and
			$$
			P(A) + P(B) - 1 \leq P(A \cap B) \leq \min\{P(A), P(B)\}
			$$
			holds.
		\end{itemize}
	\item Assume that we have a sequence of random variables, say $A, B$, and $C$, where $B$ only depends on $A$, and $C$ only depends on $B$, can you simplify the joint probability $P(A, B, C)$? Hint: this is a Markov chain.
		\begin{itemize}
			\item Note that $P(A, B)$ is just shorthand for $P(A \cap B)$.
			\begin{align*}
			P(A, B, C) &= P(A|B, C) P(B, C) \\
			&= P(A|B) P(B, C) \\
			&= P(A|B) P(B|C) P(C)
			\end{align*}
			where we have used (2.6.1) from the book, as well as because $A$ and $C$ are independent it follows that $P(A|B, C) = P(A|B)$. This can be expressed more generally, if we have a collection of random variables $X_i$ such that
			$$
			P(X_{i + 1} | X_i, X_{i - 1}, \dots, X_1) = P(X_{i + 1}| X_i),
			$$
			i.e. $X_{i + 1}$ only "remembers" the previous variable then
			$$
			P(X_n, \dots, X_1) = P(X_n|X_{n - 1})P(X_{n - 2}|X_{n - 3})\cdot \dots \cdot P(X_1|X_0)P(X_0).
			$$
		\end{itemize}
	\item In Section 2.6.5, assume that the outcomes of the two tests are not independent. In particular assume that either test on its own has a false positive rate of $10\%$ and a false negative rate of $1\%$. That is, assume that
	$$
	P(D = 1 | H = 0) = 0.1,
	$$
	and that
	$$
	P(D_1, D_2 | H = 1) = 0.02.
	$$
	\begin{enumerate}
		\item Work out the joint probability table for $D_1$ and $D_2$ given $H = 0$ based on the information you have so far.
			\begin{itemize}
				\item 
			\end{itemize}
		\item Derive the probability of the patient being positive $(H = 1)$ after one test returns positive. You can assume the same baseline probability $P(H = 1) = 0.0015$ as before.
		\item Derive the probability of the patient being positive $(H = 1)$ and both tests return positive.
	\end{enumerate}
	\item Assume that you are an asset manager for an investment bank and you have a choice of stocks $s_i$ to invest in. Your portfolio needs to add up to $1$ with weights $\alpha_i$ for each stock. The stocks have an average return $\mu = E_{s \sim P}[s]$ and covariance $\Sigma = \operatorname{Cov}_{s \sim P}[s]$.
		\begin{enumerate}
			\item Compute the expected return for a given portfolio $\alpha$.
			\item If you wanted to maximize the return of the portfolio, how should you choose your investment?
			\item Compute the variance of the portfolio.
			\item Formulate an optimization problem of maximizing the return while keeping the variance constrained to an upper bound. This is the Nobel-Prize winning Markovitz portfolio (Mangram, 2013). To solve it you will need a quadratic programming solver, something way beyond the scope of this book.
		\end{enumerate}
\end{enumerate}

\section{Documentation}

\end{document}
